{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JwR3Q8nIJlF6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from statistics import mean \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBJxxmTyJlGc",
    "outputId": "07eaaf7b-75e9-4f37-d47b-9a0d5ebd98b8"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('processedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4VMXyDUJlIO"
   },
   "outputs": [],
   "source": [
    "y = d['class'] #sets y to be class column \n",
    "X = d.iloc[:,0:(d.shape[1]-1)] #sets X to be dataset with class column removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrFwIp1YJlIY",
    "outputId": "9dcc1203-b64e-40f7-f8ea-a709b062de9c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X169</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>-38</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>-94</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>-79</td>\n",
       "      <td>...</td>\n",
       "      <td>-80</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>-59</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>-22</td>\n",
       "      <td>-22</td>\n",
       "      <td>-23</td>\n",
       "      <td>-26</td>\n",
       "      <td>-36</td>\n",
       "      <td>-42</td>\n",
       "      <td>-45</td>\n",
       "      <td>-42</td>\n",
       "      <td>-45</td>\n",
       "      <td>-49</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-18</td>\n",
       "      <td>-37</td>\n",
       "      <td>-47</td>\n",
       "      <td>-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11496</th>\n",
       "      <td>-47</td>\n",
       "      <td>-11</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>141</td>\n",
       "      <td>211</td>\n",
       "      <td>246</td>\n",
       "      <td>240</td>\n",
       "      <td>193</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>-94</td>\n",
       "      <td>-65</td>\n",
       "      <td>-33</td>\n",
       "      <td>-7</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>-13</td>\n",
       "      <td>-16</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-42</td>\n",
       "      <td>-65</td>\n",
       "      <td>-48</td>\n",
       "      <td>-61</td>\n",
       "      <td>-62</td>\n",
       "      <td>-67</td>\n",
       "      <td>-30</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>-40</td>\n",
       "      <td>-25</td>\n",
       "      <td>-9</td>\n",
       "      <td>-12</td>\n",
       "      <td>-2</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>121</td>\n",
       "      <td>135</td>\n",
       "      <td>148</td>\n",
       "      <td>143</td>\n",
       "      <td>116</td>\n",
       "      <td>86</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>62</td>\n",
       "      <td>54</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>-94</td>\n",
       "      <td>-59</td>\n",
       "      <td>-25</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11500 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  ...  X169  X170  \\\n",
       "0      135  190  229  223  192  125   55   -9  -33  -38  ...     8   -17   \n",
       "1      386  382  356  331  320  315  307  272  244  232  ...   168   164   \n",
       "2      -32  -39  -47  -37  -32  -36  -57  -73  -85  -94  ...    29    57   \n",
       "3     -105 -101  -96  -92  -89  -95 -102 -100  -87  -79  ...   -80   -82   \n",
       "4       -9  -65  -98 -102  -78  -48  -16    0  -21  -59  ...    10     4   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "11495  -22  -22  -23  -26  -36  -42  -45  -42  -45  -49  ...    20    15   \n",
       "11496  -47  -11   28   77  141  211  246  240  193  136  ...   -94   -65   \n",
       "11497   14    6  -13  -16   10   26   27   -9    4   14  ...   -42   -65   \n",
       "11498  -40  -25   -9  -12   -2   12    7   19   22   29  ...   114   121   \n",
       "11499   29   41   57   72   74   62   54   43   31   23  ...   -94   -59   \n",
       "\n",
       "       X171  X172  X173  X174  X175  X176  X177  X178  \n",
       "0       -15   -31   -77  -103  -127  -116   -83   -51  \n",
       "1       150   146   152   157   156   154   143   129  \n",
       "2        64    48    19   -12   -30   -35   -35   -36  \n",
       "3       -81   -80   -77   -85   -77   -72   -69   -65  \n",
       "4         2   -12   -32   -41   -65   -83   -89   -73  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "11495    16    12     5    -1   -18   -37   -47   -48  \n",
       "11496   -33    -7    14    27    48    77   117   170  \n",
       "11497   -48   -61   -62   -67   -30    -2    -1    -8  \n",
       "11498   135   148   143   116    86    68    59    55  \n",
       "11499   -25    -4     2     5     4    -2     2    20  \n",
       "\n",
       "[11500 rows x 178 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double checking that X does not have the label column (leaving the labels as a feature is a common mistake): \n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: MAKING X AND Y SMALLER FOR NOW \n",
    "#We'll want to use the full dataset when reporting final numbers\n",
    "#I'd be nice if we could do 5 fold CV, but that will take a very very long time if we're using the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[0:2000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.iloc[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 178)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "set4NSmUJlIi"
   },
   "outputs": [],
   "source": [
    "#Train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sW0HeTTpJlIo",
    "outputId": "efcfa6f4-9e58-44b5-fbe3-eeb5e3b6f748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       321\n",
      "           1       0.00      0.00      0.00        79\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.40      0.50      0.45       400\n",
      "weighted avg       0.64      0.80      0.71       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "##SVM \n",
    "from sklearn import svm\n",
    "modelSVM = svm.SVC(gamma=0.001, C=100.) \n",
    "modelSVM.fit(X_train, y_train)\n",
    "print(classification_report(y_test,modelSVM.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQtjJ5WsJlIv"
   },
   "outputs": [],
   "source": [
    "#80% might seem  pretty bad considering the fact that we only have two classes \n",
    "#and one of them makes up 80% of the dataset...\n",
    "#However, the precision and recall and f-score aren't 0, which means \n",
    "#the model is still learning at least.\n",
    "#Still, everything is quite low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I proceed to try some more simple non-deep learning models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.88       321\n",
      "           1       0.39      0.16      0.23        79\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.61      0.55      0.55       400\n",
      "weighted avg       0.74      0.79      0.75       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "##Logistic regression: \n",
    "from sklearn import linear_model\n",
    "modelLR = linear_model.LogisticRegression(C=1e5, max_iter=1000)    \n",
    "modelLR.fit(X_train, y_train)\n",
    "print(classification_report(y_test, modelLR.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       321\n",
      "           1       0.96      0.86      0.91        79\n",
      "\n",
      "    accuracy                           0.96       400\n",
      "   macro avg       0.96      0.93      0.94       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Random forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelRF=RandomForestClassifier(n_estimators=1000)   \n",
    "modelRF.fit(X_train, y_train)\n",
    "print(classification_report(y_test, modelRF.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Neural network: \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       321\n",
      "           1       0.89      0.68      0.77        79\n",
      "\n",
      "    accuracy                           0.92       400\n",
      "   macro avg       0.91      0.83      0.86       400\n",
      "weighted avg       0.92      0.92      0.92       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelNeuralNet = MLPClassifier(hidden_layer_sizes=(5,5,5), max_iter=500) \n",
    "#3 hidden layers with 13 neurons in each layer \n",
    "modelNeuralNet.fit(X_train, y_train)\n",
    "print(classification_report(y_test,modelNeuralNet.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitely room for improvement on all 4 baselines.\n",
    "#The slower ones (RF, NN) have higher performance as expected\n",
    "#I tried out some other models. kNN performs similarly to SVM and logistic regression, \n",
    "#and adaboost and GBMs perform similarly to the random forest. \n",
    "#But to keep our presentation from getting cluttered, let's just stick to these\n",
    "#4 baseline models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validations (could only do 3 fold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_cv_results = cross_validate(modelSVM, X, y, cv=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "LR_cv_results = cross_validate(modelLR, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNet_cv_results = cross_validate(modelNeuralNet, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_cv_results = cross_validate(modelRF, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "SVM_cv_results: 0.8040004022013018\n",
      "LR_cv_results: 0.8210031620826224\n",
      "NeuralNet_cv_results: 0.8195061628344987\n",
      "RandomForest_cv_results: 0.9620002311156733\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracies:\")\n",
    "print(\"SVM_cv_results: \" + str(mean(SVM_cv_results['test_score'])))\n",
    "print(\"LR_cv_results: \" + str(mean(LR_cv_results['test_score'])))\n",
    "print(\"NeuralNet_cv_results: \" + str(mean(NeuralNet_cv_results['test_score'])))\n",
    "print(\"RandomForest_cv_results: \" + str(mean(RF_cv_results['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training times:\n",
      "SVM_cv_results: 0.5205260912577311\n",
      "LR_cv_results: 0.6916908423105875\n",
      "NeuralNet_cv_results: 2.470816214879354\n",
      "RandomForest_cv_results: 12.549350102742514\n"
     ]
    }
   ],
   "source": [
    "print(\"Training times:\")\n",
    "print(\"SVM_cv_results: \" + str(mean(SVM_cv_results['fit_time'])))\n",
    "print(\"LR_cv_results: \" + str(mean(LR_cv_results['fit_time'])))\n",
    "print(\"NeuralNet_cv_results: \" + str(mean(NeuralNet_cv_results['fit_time'])))\n",
    "print(\"RandomForest_cv_results: \" + str(mean(RF_cv_results['fit_time'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction times:\n",
      "SVM_cv_results: 0.230975071589152\n",
      "LR_cv_results: 0.0026712417602539062\n",
      "NeuralNet_cv_results: 0.006540616353352864\n",
      "RandomForest_cv_results: 0.18070538838704428\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction times:\")\n",
    "print(\"SVM_cv_results: \" + str(mean(SVM_cv_results['score_time'])))\n",
    "print(\"LR_cv_results: \" + str(mean(LR_cv_results['score_time'])))\n",
    "print(\"NeuralNet_cv_results: \" + str(mean(NeuralNet_cv_results['score_time'])))\n",
    "print(\"RandomForest_cv_results: \" + str(mean(RF_cv_results['score_time'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphingY=[mean(SVM_cv_results['test_score']), \n",
    "   mean(LR_cv_results['test_score']),\n",
    "   mean(NeuralNet_cv_results['test_score']),\n",
    "   mean(RF_cv_results['test_score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xLabels=(\"SVM\",\"LogisticR\",\"NeuralNet\",\"RF\")\n",
    "x = np.arange(4)\n",
    "\n",
    "colours = ['turquoise','lightseagreen','teal',\"darkcyan\"]\n",
    "plt.bar(x, graphingY, width=0.5, color=colours)\n",
    "plt.xticks(x, xLabels, rotation=45)\n",
    "\n",
    "plt.title(\"Accuracies from CV on raw data:\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "SVM_cv_results: 0.8040004022013018\n",
      "LR_cv_results: 0.8210031620826224\n",
      "NeuralNet_cv_results: 0.8240024132078105\n",
      "RandomForest_cv_results: 0.9650002326164245\n",
      "\n",
      "Training times:\n",
      "SVM_cv_results: 0.5881137053171793\n",
      "LR_cv_results: 0.7048002878824869\n",
      "NeuralNet_cv_results: 2.552797476450602\n",
      "RandomForest_cv_results: 12.793964306513468\n",
      "\n",
      "Prediction times:\n",
      "SVM_cv_results: 0.2587850093841553\n",
      "LR_cv_results: 0.0\n",
      "NeuralNet_cv_results: 0.009215195973714193\n",
      "RandomForest_cv_results: 0.21432622273763022\n"
     ]
    }
   ],
   "source": [
    "#The cross validation stuff is in reportingFramework.py\n",
    "#In the other notebooks, we'll just use this method to keep things concise\n",
    "\n",
    "from CVreportingFramework import hugeFramework\n",
    "hugeFramework(modelSVM, modelLR, modelNeuralNet, modelRF, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "initialProcessingAndAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
