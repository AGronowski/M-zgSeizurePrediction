{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JwR3Q8nIJlF6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from statistics import mean \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBJxxmTyJlGc",
    "outputId": "07eaaf7b-75e9-4f37-d47b-9a0d5ebd98b8"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('processedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4VMXyDUJlIO"
   },
   "outputs": [],
   "source": [
    "y = d['class'] #sets y to be class column \n",
    "X = d.iloc[:,0:(d.shape[1]-1)] #sets X to be dataset with class column removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrFwIp1YJlIY",
    "outputId": "9dcc1203-b64e-40f7-f8ea-a709b062de9c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X169</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>-38</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>-94</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>-79</td>\n",
       "      <td>...</td>\n",
       "      <td>-80</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>-59</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>-22</td>\n",
       "      <td>-22</td>\n",
       "      <td>-23</td>\n",
       "      <td>-26</td>\n",
       "      <td>-36</td>\n",
       "      <td>-42</td>\n",
       "      <td>-45</td>\n",
       "      <td>-42</td>\n",
       "      <td>-45</td>\n",
       "      <td>-49</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-18</td>\n",
       "      <td>-37</td>\n",
       "      <td>-47</td>\n",
       "      <td>-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11496</th>\n",
       "      <td>-47</td>\n",
       "      <td>-11</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>141</td>\n",
       "      <td>211</td>\n",
       "      <td>246</td>\n",
       "      <td>240</td>\n",
       "      <td>193</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>-94</td>\n",
       "      <td>-65</td>\n",
       "      <td>-33</td>\n",
       "      <td>-7</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>-13</td>\n",
       "      <td>-16</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-42</td>\n",
       "      <td>-65</td>\n",
       "      <td>-48</td>\n",
       "      <td>-61</td>\n",
       "      <td>-62</td>\n",
       "      <td>-67</td>\n",
       "      <td>-30</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>-40</td>\n",
       "      <td>-25</td>\n",
       "      <td>-9</td>\n",
       "      <td>-12</td>\n",
       "      <td>-2</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>121</td>\n",
       "      <td>135</td>\n",
       "      <td>148</td>\n",
       "      <td>143</td>\n",
       "      <td>116</td>\n",
       "      <td>86</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>62</td>\n",
       "      <td>54</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>-94</td>\n",
       "      <td>-59</td>\n",
       "      <td>-25</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11500 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  ...  X169  X170  \\\n",
       "0      135  190  229  223  192  125   55   -9  -33  -38  ...     8   -17   \n",
       "1      386  382  356  331  320  315  307  272  244  232  ...   168   164   \n",
       "2      -32  -39  -47  -37  -32  -36  -57  -73  -85  -94  ...    29    57   \n",
       "3     -105 -101  -96  -92  -89  -95 -102 -100  -87  -79  ...   -80   -82   \n",
       "4       -9  -65  -98 -102  -78  -48  -16    0  -21  -59  ...    10     4   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "11495  -22  -22  -23  -26  -36  -42  -45  -42  -45  -49  ...    20    15   \n",
       "11496  -47  -11   28   77  141  211  246  240  193  136  ...   -94   -65   \n",
       "11497   14    6  -13  -16   10   26   27   -9    4   14  ...   -42   -65   \n",
       "11498  -40  -25   -9  -12   -2   12    7   19   22   29  ...   114   121   \n",
       "11499   29   41   57   72   74   62   54   43   31   23  ...   -94   -59   \n",
       "\n",
       "       X171  X172  X173  X174  X175  X176  X177  X178  \n",
       "0       -15   -31   -77  -103  -127  -116   -83   -51  \n",
       "1       150   146   152   157   156   154   143   129  \n",
       "2        64    48    19   -12   -30   -35   -35   -36  \n",
       "3       -81   -80   -77   -85   -77   -72   -69   -65  \n",
       "4         2   -12   -32   -41   -65   -83   -89   -73  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "11495    16    12     5    -1   -18   -37   -47   -48  \n",
       "11496   -33    -7    14    27    48    77   117   170  \n",
       "11497   -48   -61   -62   -67   -30    -2    -1    -8  \n",
       "11498   135   148   143   116    86    68    59    55  \n",
       "11499   -25    -4     2     5     4    -2     2    20  \n",
       "\n",
       "[11500 rows x 178 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double checking that X does not have the label column (leaving the labels as a feature is a common mistake): \n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: MAKING X AND Y SMALLER FOR NOW \n",
    "#We'll want to use the full dataset when reporting final numbers\n",
    "#I'd be nice if we could do 5 fold CV, but that will take a very very long time if we're using the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[0:2000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.iloc[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 178)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "set4NSmUJlIi"
   },
   "outputs": [],
   "source": [
    "#Train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sW0HeTTpJlIo",
    "outputId": "efcfa6f4-9e58-44b5-fbe3-eeb5e3b6f748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89       323\n",
      "           1       0.00      0.00      0.00        77\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.40      0.50      0.45       400\n",
      "weighted avg       0.65      0.81      0.72       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "##SVM \n",
    "from sklearn import svm\n",
    "modelSVM = svm.SVC(gamma=0.001, C=100.) \n",
    "modelSVM.fit(X_train, y_train)\n",
    "print(classification_report(y_test,modelSVM.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQtjJ5WsJlIv"
   },
   "outputs": [],
   "source": [
    "#80% might seem  pretty bad considering the fact that we only have two classes \n",
    "#and one of them makes up 80% of the dataset...\n",
    "#However, the precision and recall and f-score aren't 0, which means \n",
    "#the model is still learning at least.\n",
    "#Still, everything is quite low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I proceed to try some more simple non-deep learning models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91       323\n",
      "           1       0.74      0.26      0.38        77\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.79      0.62      0.65       400\n",
      "weighted avg       0.83      0.84      0.81       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "##Logistic regression: \n",
    "from sklearn import linear_model\n",
    "modelLR = linear_model.LogisticRegression(C=1e5, max_iter=1000)    \n",
    "modelLR.fit(X_train, y_train)\n",
    "print(classification_report(y_test, modelLR.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       323\n",
      "           1       0.91      0.92      0.92        77\n",
      "\n",
      "    accuracy                           0.97       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Random forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelRF=RandomForestClassifier(n_estimators=1000)   \n",
    "modelRF.fit(X_train, y_train)\n",
    "print(classification_report(y_test, modelRF.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Neural network: \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       323\n",
      "           1       0.86      0.71      0.78        77\n",
      "\n",
      "    accuracy                           0.92       400\n",
      "   macro avg       0.90      0.84      0.87       400\n",
      "weighted avg       0.92      0.92      0.92       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelNeuralNet = MLPClassifier(hidden_layer_sizes=(5,5,5), max_iter=500) \n",
    "#3 hidden layers with 13 neurons in each layer \n",
    "modelNeuralNet.fit(X_train, y_train)\n",
    "print(classification_report(y_test,modelNeuralNet.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitely room for improvement on all 4 baselines.\n",
    "#The slower ones (RF, NN) have higher performance as expected\n",
    "#I tried out some other models. kNN performs similarly to SVM and logistic regression, \n",
    "#and adaboost and GBMs perform similarly to the random forest. \n",
    "#But to keep our presentation from getting cluttered, let's just stick to these\n",
    "#4 baseline models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validations (could only do 3 fold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_cv_results = cross_validate(modelSVM, X, y, cv=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "LR_cv_results = cross_validate(modelLR, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNet_cv_results = cross_validate(modelNeuralNet, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_cv_results = cross_validate(modelRF, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "SVM_cv_results: 0.8040004022013018\n",
      "LR_cv_results: 0.8210031620826224\n",
      "NeuralNet_cv_results: 0.8615031823427626\n",
      "RandomForest_cv_results: 0.963500982241612\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracies:\")\n",
    "print(\"SVM_cv_results: \" + str(mean(SVM_cv_results['test_score'])))\n",
    "print(\"LR_cv_results: \" + str(mean(LR_cv_results['test_score'])))\n",
    "print(\"NeuralNet_cv_results: \" + str(mean(NeuralNet_cv_results['test_score'])))\n",
    "print(\"RandomForest_cv_results: \" + str(mean(RF_cv_results['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training times:\n",
      "SVM_cv_results: 0.5467367172241211\n",
      "LR_cv_results: 0.7000529766082764\n",
      "NeuralNet_cv_results: 1.9772624969482422\n",
      "RandomForest_cv_results: 13.372642993927002\n"
     ]
    }
   ],
   "source": [
    "print(\"Training times:\")\n",
    "print(\"SVM_cv_results: \" + str(mean(SVM_cv_results['fit_time'])))\n",
    "print(\"LR_cv_results: \" + str(mean(LR_cv_results['fit_time'])))\n",
    "print(\"NeuralNet_cv_results: \" + str(mean(NeuralNet_cv_results['fit_time'])))\n",
    "print(\"RandomForest_cv_results: \" + str(mean(RF_cv_results['fit_time'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction times:\n",
      "SVM_cv_results: 0.23175621032714844\n",
      "LR_cv_results: 0.0\n",
      "NeuralNet_cv_results: 0.003428220748901367\n",
      "RandomForest_cv_results: 0.22148569424947104\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction times:\")\n",
    "print(\"SVM_cv_results: \" + str(mean(SVM_cv_results['score_time'])))\n",
    "print(\"LR_cv_results: \" + str(mean(LR_cv_results['score_time'])))\n",
    "print(\"NeuralNet_cv_results: \" + str(mean(NeuralNet_cv_results['score_time'])))\n",
    "print(\"RandomForest_cv_results: \" + str(mean(RF_cv_results['score_time'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[mean(SVM_cv_results['test_score']), \n",
    "   mean(LR_cv_results['test_score']),\n",
    "   mean(NeuralNet_cv_results['test_score']),\n",
    "   mean(RF_cv_results['test_score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xLabels=(\"SVM\",\"LogisticR\",\"NeuralNet\",\"RF\")\n",
    "x = np.arange(4)\n",
    "\n",
    "colours = ['turquoise','lightseagreen','teal',\"darkcyan\"]\n",
    "plt.bar(x, y, width=0.5, color=colours)\n",
    "plt.xticks(x, xLabels, rotation=45)\n",
    "\n",
    "plt.title(\"Accuracies from CV on raw data:\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "initialProcessingAndAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
