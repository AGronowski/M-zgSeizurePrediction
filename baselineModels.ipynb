{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JwR3Q8nIJlF6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from statistics import mean \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBJxxmTyJlGc",
    "outputId": "07eaaf7b-75e9-4f37-d47b-9a0d5ebd98b8"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('processedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4VMXyDUJlIO"
   },
   "outputs": [],
   "source": [
    "y = d['class'] #sets y to be class column \n",
    "X = d.iloc[:,0:(d.shape[1]-1)] #sets X to be dataset with class column removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrFwIp1YJlIY",
    "outputId": "9dcc1203-b64e-40f7-f8ea-a709b062de9c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X169</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>-38</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>-94</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>-79</td>\n",
       "      <td>...</td>\n",
       "      <td>-80</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>-59</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>-22</td>\n",
       "      <td>-22</td>\n",
       "      <td>-23</td>\n",
       "      <td>-26</td>\n",
       "      <td>-36</td>\n",
       "      <td>-42</td>\n",
       "      <td>-45</td>\n",
       "      <td>-42</td>\n",
       "      <td>-45</td>\n",
       "      <td>-49</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-18</td>\n",
       "      <td>-37</td>\n",
       "      <td>-47</td>\n",
       "      <td>-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11496</th>\n",
       "      <td>-47</td>\n",
       "      <td>-11</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>141</td>\n",
       "      <td>211</td>\n",
       "      <td>246</td>\n",
       "      <td>240</td>\n",
       "      <td>193</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>-94</td>\n",
       "      <td>-65</td>\n",
       "      <td>-33</td>\n",
       "      <td>-7</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>-13</td>\n",
       "      <td>-16</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-42</td>\n",
       "      <td>-65</td>\n",
       "      <td>-48</td>\n",
       "      <td>-61</td>\n",
       "      <td>-62</td>\n",
       "      <td>-67</td>\n",
       "      <td>-30</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>-40</td>\n",
       "      <td>-25</td>\n",
       "      <td>-9</td>\n",
       "      <td>-12</td>\n",
       "      <td>-2</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>121</td>\n",
       "      <td>135</td>\n",
       "      <td>148</td>\n",
       "      <td>143</td>\n",
       "      <td>116</td>\n",
       "      <td>86</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>62</td>\n",
       "      <td>54</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>-94</td>\n",
       "      <td>-59</td>\n",
       "      <td>-25</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11500 rows Ã— 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  ...  X169  X170  \\\n",
       "0      135  190  229  223  192  125   55   -9  -33  -38  ...     8   -17   \n",
       "1      386  382  356  331  320  315  307  272  244  232  ...   168   164   \n",
       "2      -32  -39  -47  -37  -32  -36  -57  -73  -85  -94  ...    29    57   \n",
       "3     -105 -101  -96  -92  -89  -95 -102 -100  -87  -79  ...   -80   -82   \n",
       "4       -9  -65  -98 -102  -78  -48  -16    0  -21  -59  ...    10     4   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "11495  -22  -22  -23  -26  -36  -42  -45  -42  -45  -49  ...    20    15   \n",
       "11496  -47  -11   28   77  141  211  246  240  193  136  ...   -94   -65   \n",
       "11497   14    6  -13  -16   10   26   27   -9    4   14  ...   -42   -65   \n",
       "11498  -40  -25   -9  -12   -2   12    7   19   22   29  ...   114   121   \n",
       "11499   29   41   57   72   74   62   54   43   31   23  ...   -94   -59   \n",
       "\n",
       "       X171  X172  X173  X174  X175  X176  X177  X178  \n",
       "0       -15   -31   -77  -103  -127  -116   -83   -51  \n",
       "1       150   146   152   157   156   154   143   129  \n",
       "2        64    48    19   -12   -30   -35   -35   -36  \n",
       "3       -81   -80   -77   -85   -77   -72   -69   -65  \n",
       "4         2   -12   -32   -41   -65   -83   -89   -73  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "11495    16    12     5    -1   -18   -37   -47   -48  \n",
       "11496   -33    -7    14    27    48    77   117   170  \n",
       "11497   -48   -61   -62   -67   -30    -2    -1    -8  \n",
       "11498   135   148   143   116    86    68    59    55  \n",
       "11499   -25    -4     2     5     4    -2     2    20  \n",
       "\n",
       "[11500 rows x 178 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double checking that X does not have the label column (leaving the labels as a feature is a common mistake): \n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: MAKING X AND Y SMALLER FOR NOW \n",
    "#We'll want to use the full dataset when reporting final numbers\n",
    "#I'd be nice if we could do 5 fold CV, but that will take a very very long time if we're using the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[0:2000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.iloc[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 178)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "set4NSmUJlIi"
   },
   "outputs": [],
   "source": [
    "#Train test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sW0HeTTpJlIo",
    "outputId": "efcfa6f4-9e58-44b5-fbe3-eeb5e3b6f748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       317\n",
      "           1       0.00      0.00      0.00        83\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.40      0.50      0.44       400\n",
      "weighted avg       0.63      0.79      0.70       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "##SVM \n",
    "from sklearn import svm\n",
    "modelSVM = svm.SVC(gamma=0.001, C=100.) \n",
    "modelSVM.fit(X_train, y_train)\n",
    "print(classification_report(y_test,modelSVM.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQtjJ5WsJlIv"
   },
   "outputs": [],
   "source": [
    "#80% might seem  pretty bad considering the fact that we only have two classes \n",
    "#and one of them makes up 80% of the dataset...\n",
    "#However, the precision and recall and f-score aren't 0, which means \n",
    "#the model is still learning at least.\n",
    "#Still, everything is quite low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I proceed to try some more simple non-deep learning models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       317\n",
      "           1       0.71      0.35      0.47        83\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.78      0.66      0.69       400\n",
      "weighted avg       0.82      0.83      0.81       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "##Logistic regression: \n",
    "from sklearn import linear_model\n",
    "modelLR = linear_model.LogisticRegression(C=1e5, max_iter=1000)    \n",
    "modelLR.fit(X_train, y_train)\n",
    "print(classification_report(y_test, modelLR.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       317\n",
      "           1       0.93      0.89      0.91        83\n",
      "\n",
      "    accuracy                           0.96       400\n",
      "   macro avg       0.95      0.94      0.94       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Random forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelRF=RandomForestClassifier(n_estimators=1000)   \n",
    "modelRF.fit(X_train, y_train)\n",
    "print(classification_report(y_test, modelRF.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Neural network: \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       317\n",
      "           1       0.96      0.78      0.86        83\n",
      "\n",
      "    accuracy                           0.95       400\n",
      "   macro avg       0.95      0.89      0.91       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelNeuralNet = MLPClassifier(hidden_layer_sizes=(5,5,5), max_iter=500) \n",
    "#3 hidden layers with 13 neurons in each layer \n",
    "modelNeuralNet.fit(X_train, y_train)\n",
    "print(classification_report(y_test,modelNeuralNet.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitely room for improvement on all 4 baselines.\n",
    "#The slower ones (RF, NN) have higher performance as expected\n",
    "#I tried out some other models. kNN performs similarly to SVM and logistic regression, \n",
    "#and adaboost and GBMs perform similarly to the random forest. \n",
    "#But to keep our presentation from getting cluttered, let's just stick to these\n",
    "#4 baseline models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validations (could only do 3 fold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_cv_results = cross_validate(modelSVM, X, y, cv=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "LR_cv_results = cross_validate(modelLR, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNet_cv_results = cross_validate(modelNeuralNet, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_cv_results = cross_validate(modelRF, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "SVM_cv_results: 0.8040004022013018\n",
      "LR_cv_results: 0.8210031620826224\n",
      "NeuralNet_cv_results: 0.8535034284659472\n",
      "RandomForest_cv_results: 0.9610007308657983\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracies:\")\n",
    "print(\"SVM_cv_results: \" + str(mean(SVM_cv_results['test_score'])))\n",
    "print(\"LR_cv_results: \" + str(mean(LR_cv_results['test_score'])))\n",
    "print(\"NeuralNet_cv_results: \" + str(mean(NeuralNet_cv_results['test_score'])))\n",
    "print(\"RandomForest_cv_results: \" + str(mean(RF_cv_results['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training times:\n",
      "SVM_cv_results: 0.4973607858022054\n",
      "LR_cv_results: 0.7760240236918131\n",
      "NeuralNet_cv_results: 2.5670727094014487\n",
      "RandomForest_cv_results: 11.670895099639893\n"
     ]
    }
   ],
   "source": [
    "print(\"Training times:\")\n",
    "print(\"SVM_cv_results: \" + str(mean(SVM_cv_results['fit_time'])))\n",
    "print(\"LR_cv_results: \" + str(mean(LR_cv_results['fit_time'])))\n",
    "print(\"NeuralNet_cv_results: \" + str(mean(NeuralNet_cv_results['fit_time'])))\n",
    "print(\"RandomForest_cv_results: \" + str(mean(RF_cv_results['fit_time'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction times:\n",
      "SVM_cv_results: 0.22657879193623862\n",
      "LR_cv_results: 0.0\n",
      "NeuralNet_cv_results: 0.0\n",
      "RandomForest_cv_results: 0.18694194157918295\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction times:\")\n",
    "print(\"SVM_cv_results: \" + str(mean(SVM_cv_results['score_time'])))\n",
    "print(\"LR_cv_results: \" + str(mean(LR_cv_results['score_time'])))\n",
    "print(\"NeuralNet_cv_results: \" + str(mean(NeuralNet_cv_results['score_time'])))\n",
    "print(\"RandomForest_cv_results: \" + str(mean(RF_cv_results['score_time'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[mean(SVM_cv_results['test_score']), \n",
    "   mean(LR_cv_results['test_score']),\n",
    "   mean(NeuralNet_cv_results['test_score']),\n",
    "   mean(RF_cv_results['test_score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xLabels=(\"SVM\",\"LogisticR\",\"NeuralNet\",\"RF\")\n",
    "x = np.arange(4)\n",
    "\n",
    "colours = ['turquoise','lightseagreen','teal',\"darkcyan\"]\n",
    "plt.bar(x, y, width=0.5, color=colours)\n",
    "plt.xticks(x, xLabels, rotation=45)\n",
    "\n",
    "plt.title(\"Accuracies from CV on raw data:\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer to reportingFramework.py to see what hugeFramework() does. It's very simple\n",
    "#just runs cross validation on the 4 models passed to it and prints results\n",
    "from reportingFramework import hugeFramework\n",
    "\n",
    "hugeFramework(modelSVM, modelLR, modelNeuralNet, modelRF, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the other notebooks, I'll just use this method to keep things concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "initialProcessingAndAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
